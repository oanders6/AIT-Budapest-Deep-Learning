{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oanders6/AIT-Budapest-Deep-Learning/blob/main/Anderson_Owen_AIT_RAG_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce4-y4JJgVSv"
      },
      "source": [
        "# RAG Assesment\n",
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * **latest scientific papers from arxiv.org,** ***selected***\n",
        "   * fiction books released,\n",
        "   * legal documents or,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points) ***all were released May 1st, 2024***\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points) ***see prompts below/in testing sections***\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points) ***LLaMa 3 8B was loaded***\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points) ***see Testing section–LLM answers were either incorrect, or it could not come up with an answer***\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points) ***see \"Using LangChain RetrievalQA and establishing RAG template\" section***\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points) ***see Testing section at bottom of notebook***\n"
      ],
      "metadata": {
        "id": "JucXu2SAPIqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts:\n",
        "\n",
        "**Relevant**:\n",
        "1.   Who authored the paper titled: Quantum algorithms for matrix geometric means that was released in 2024?\n",
        "2.   What is the name of the university Runyi Yang worked at when they published the paper titled: SUNDAE: Spectrally Pruned Gaussian Fields with Neural Compensation? Do not provide any information other than the name. If you do not recognize the paper, then do not provide a name.\n",
        "3.   Could you provide me with the table of contents for Charlotte Chan's paper 'The Scalar Product Formula for Parahoric Deligne–Lusztig Induction'?\n",
        "\n",
        "**Irrelevant**:\n",
        "\n",
        "\n",
        "1.   Accoridng to her paper, what is Charlotte Chan's favorite color?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M6D89ZqXQREd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPpiHVgj4CmG"
      },
      "source": [
        "## Installing dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zjLkm65J_S0v"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate > null\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30AIKqF9rTuv"
      },
      "source": [
        "# Restarting kernel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "qEb7_aQ50Y-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X22Ccat1oXE2"
      },
      "source": [
        "# Importing the model (I chose LLaMa 3 8B)\n",
        "Next, we import the necessary Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PatG1bZXGOws",
        "outputId": "167865a5-2dc0-491a-aa85-212682496b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# you need to define your private User Access Token from Huggingface\n",
        "# to be able to access models with accepted licence\n",
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_n7y1VBMHGk"
      },
      "source": [
        "Next, we download a model from Huggingface. You can find two variants - a smaller and a larger one. Let's start with the smaller, and later make experiments with the larger one too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N_hQ_IfuoWT6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "bc79a48161a94787a68ba6f45d4ab0b1",
            "b4b0fd038465494ab7829fca7675efb7",
            "55000a1e9c6942c2adfa0a005d542dd0",
            "3856536bedc84df8aee1190f0989496d",
            "230cde4b359d4ac58429e52ec70ebeb8",
            "722824c817164935a178c51182d6c698",
            "3afe89a6bbf34503bd4a82f705de1e18",
            "309b06e039cd413484ba7b4739f20307",
            "51a4440f95ec4f3993e3ccae3e7697af",
            "5bf7270b096d424986717788035e5bbd",
            "a3eecc4a0beb4326b0c48c90253b52ca"
          ]
        },
        "outputId": "eb1f6065-73ff-4b46-a596-d03c08124f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc79a48161a94787a68ba6f45d4ab0b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # 8B language model from Meta AI\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0 # For RAG we would like to have determenistic answers\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Templates"
      ],
      "metadata": {
        "id": "T1yErenCHr0n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BX6bnnL45a2N"
      },
      "outputs": [],
      "source": [
        "template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "template=template_llama3\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing (LLM)"
      ],
      "metadata": {
        "id": "v3QfN7ivJZE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is the name of the university Runyi Yang worked at when they published the paper titled: SUNDAE: Spectrally Pruned Gaussian Fields with Neural Compensation? If you do not recognize the paper, then do not provide a name.\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYOPye9bJcDC",
        "outputId": "ecbc0b01-2353-48dd-f12a-5876ffb399ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  What is the name of the university Runyi\n",
            "Yang worked at when they published the paper titled: SUNDAE: Spectrally Pruned Gaussian Fields with\n",
            "Neural Compensation? If you do not recognize the paper, then do not provide a\n",
            "name.<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but I couldn't find any\n",
            "information on a person named Runyi Yang publishing a paper called \"SUNDAE: Spectrally Pruned\n",
            "Gaussian Fields with Neural Compensation\". It's possible that the paper doesn't exist or is not\n",
            "well-known. Can you provide more context or details about the paper or author?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Who authored the paper titled: Quantum algorithms for matrix geometric means that was released in 2024?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuuTfrBr2FOV",
        "outputId": "fc9288df-01d7-497e-be5a-aa071e92f670"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  Who authored the paper titled: Quantum\n",
            "algorithms for matrix geometric means that was released in\n",
            "2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I apologize, but I couldn't find any\n",
            "information on a paper titled \"Quantum algorithms for matrix geometric means\" published in 2024.\n",
            "It's possible that the paper has not been published yet or is not widely known.  However, there are\n",
            "several papers on quantum algorithms for computing matrix geometric means that have been published\n",
            "in recent years. For example:  * \"Quantum algorithms for matrix geometric means and their\n",
            "applications\" by Wang et al., published in 2020 in the Journal of Physics A. * \"Fast quantum\n",
            "algorithms for matrix geometric means\" by Li et al., published in 2019 in the Journal of\n",
            "Mathematical Physics. * \"Quantum algorithms for matrix exponentials and logarithms\" by Childs et\n",
            "al., published in 2017 in the Journal of the ACM.  If you could provide more context or details\n",
            "about the paper you're looking for, I may be able to help you identify it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Could you provide me with the table of contents for Charlotte Chan's paper 'The Scalar Product Formula for Parahoric Deligne–Lusztig Induction'?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnq5K0gMKBgY",
        "outputId": "dad27c7b-3a03-4c9b-9966-42beb3ede8be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>  Could you provide me with the table of\n",
            "contents for Charlotte Chan's paper 'The Scalar Product Formula for Parahoric Deligne–Lusztig\n",
            "Induction'?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I'm happy to help! However, I\n",
            "have to inform you that I couldn't find any information on a paper called \"The Scalar Product\n",
            "Formula for Parahoric Deligne–Lusztig Induction\" by Charlotte Chan.  It's possible that the paper\n",
            "doesn't exist or is not widely known. Could you please provide more context or details about the\n",
            "paper, such as where you heard about it or what journal it was published in? This might help me\n",
            "assist you better.  Alternatively, if you're looking for information on parahoric Deligne-Lusztig\n",
            "induction or scalar product formulas in representation theory, I'd be happy to try and provide some\n",
            "general resources or references. Just let me know how I can help!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k03r4pKIcyAl"
      },
      "source": [
        "# Loading embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R3hqEWvUcyBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8744e3df-751c-4821-d6e3-a14f2030e2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings() # default model_name=\"sentence-transformers/all-mpnet-base-v2\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agfSAbN_zvYs"
      },
      "source": [
        "# Using LangChain RetrievalQA and establishing RAG template\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxfNkw_PnItp",
        "outputId": "76ab70e6-3a32-4ce7-9795-49984c1041e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 9.78 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "prompt_template_llama3 = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt_template=prompt_template_llama3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgSv0RrUPaDW"
      },
      "source": [
        "# Loading PDFs\n",
        "----\n",
        "As per the assignment details, I loaded 8 academic papers from arxiv.org. These papers were all published on May 1st, 2024, and thus were not included in LLaMa 3's training data (as LLaMa 3 was published on April 18th, 2024).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw_5w2c-PjeF",
        "outputId": "9a2702c9-f195-4ac9-8d9d-d02fabd99d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-03 16:31:52--  https://arxiv.org/pdf/2405.00676\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.131.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13547625 (13M) [application/pdf]\n",
            "Saving to: ‘document1.pdf’\n",
            "\n",
            "document1.pdf       100%[===================>]  12.92M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-05-03 16:31:52 (121 MB/s) - ‘document1.pdf’ saved [13547625/13547625]\n",
            "\n",
            "--2024-05-03 16:31:52--  https://arxiv.org/pdf/2405.00675\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 754757 (737K) [application/pdf]\n",
            "Saving to: ‘document2.pdf’\n",
            "\n",
            "document2.pdf       100%[===================>] 737.07K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-03 16:31:52 (28.2 MB/s) - ‘document2.pdf’ saved [754757/754757]\n",
            "\n",
            "--2024-05-03 16:31:52--  https://arxiv.org/pdf/2405.00674\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 593298 (579K) [application/pdf]\n",
            "Saving to: ‘document3.pdf’\n",
            "\n",
            "document3.pdf       100%[===================>] 579.39K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-03 16:31:53 (19.9 MB/s) - ‘document3.pdf’ saved [593298/593298]\n",
            "\n",
            "--2024-05-03 16:31:53--  https://arxiv.org/pdf/2405.00673\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 786642 (768K) [application/pdf]\n",
            "Saving to: ‘document4.pdf’\n",
            "\n",
            "document4.pdf       100%[===================>] 768.21K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-03 16:31:53 (25.4 MB/s) - ‘document4.pdf’ saved [786642/786642]\n",
            "\n",
            "--2024-05-03 16:31:53--  https://arxiv.org/pdf/2405.00672\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30060633 (29M) [application/pdf]\n",
            "Saving to: ‘document5.pdf’\n",
            "\n",
            "document5.pdf       100%[===================>]  28.67M   175MB/s    in 0.2s    \n",
            "\n",
            "2024-05-03 16:31:53 (175 MB/s) - ‘document5.pdf’ saved [30060633/30060633]\n",
            "\n",
            "--2024-05-03 16:31:53--  https://arxiv.org/pdf/2405.00671\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 340187 (332K) [application/pdf]\n",
            "Saving to: ‘document6.pdf’\n",
            "\n",
            "document6.pdf       100%[===================>] 332.21K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-03 16:31:53 (11.6 MB/s) - ‘document6.pdf’ saved [340187/340187]\n",
            "\n",
            "--2024-05-03 16:31:53--  https://arxiv.org/pdf/2405.00670\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 455965 (445K) [application/pdf]\n",
            "Saving to: ‘document7.pdf’\n",
            "\n",
            "document7.pdf       100%[===================>] 445.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-03 16:31:53 (19.7 MB/s) - ‘document7.pdf’ saved [455965/455965]\n",
            "\n",
            "--2024-05-03 16:31:53--  https://arxiv.org/pdf/2405.00669\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4025607 (3.8M) [application/pdf]\n",
            "Saving to: ‘document8.pdf’\n",
            "\n",
            "document8.pdf       100%[===================>]   3.84M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-03 16:31:53 (91.6 MB/s) - ‘document8.pdf’ saved [4025607/4025607]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# All of these papers were released in May 2024\n",
        "!wget -O document1.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00676\"\n",
        "!wget -O document2.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00675\"\n",
        "!wget -O document3.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00674\"\n",
        "!wget -O document4.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00673\"\n",
        "!wget -O document5.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00672\"\n",
        "!wget -O document6.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00671\"\n",
        "!wget -O document7.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00670\"\n",
        "!wget -O document8.pdf --no-check-certificate \"https://arxiv.org/pdf/2405.00669\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2O0JxHkUQsHg"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "import os\n",
        "loaders = [UnstructuredPDFLoader(fn) for fn in [\"/content/document1.pdf\",\"/content/document2.pdf\", \"/content/document3.pdf\", \"/content/document4.pdf\", \"/content/document5.pdf\", \"/content/document6.pdf\", \"/content/document7.pdf\", \"/content/document8.pdf\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEfY__rf4BjS"
      },
      "source": [
        "As before, we split it into smaller pieces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--JjeRqGTwH-",
        "outputId": "9e88173f-3793-48d8-98c9-fa4e86a11226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw document.../content/document1.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document2.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document3.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document4.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document5.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document6.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document7.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/document8.pdf\n",
            "Splitting text...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "897"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunked_pdf_doc = []\n",
        "\n",
        "for loader in loaders:\n",
        "    print(\"Loading raw document...\" + loader.file_path)\n",
        "    pdf_doc = loader.load()\n",
        "    updated_pdf_doc = filter_complex_metadata(pdf_doc)\n",
        "    print(\"Splitting text...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "    documents = text_splitter.split_documents(updated_pdf_doc)\n",
        "    chunked_pdf_doc.extend(documents)\n",
        "\n",
        "len(chunked_pdf_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4s2c_Q547N6"
      },
      "source": [
        "Make the vectors for each part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXDUlIAsUsRJ",
        "outputId": "4479edeb-c8a1-492e-d1f3-7c93bf62dd7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 19.6 s, sys: 48 ms, total: 19.6 s\n",
            "Wall time: 20 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing (LLM + RAG)"
      ],
      "metadata": {
        "id": "-4meFH3OP6Er"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-InFSJZVtTu",
        "outputId": "3f9a1c87-bd45-41d6-a99a-d7319a1b9294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to Answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  4 2 0 2  y a M 1  ]  V C . s c [  1 v 6 7 6 0 0 . 5 0 4 2 : v i X r a  SUNDAE:\n",
            "Spectrally Pruned Gaussian Fields with Neural Compensation  Runyi Yang AIR, Tsinghua University\n",
            "Imperial College London runyi.yang23@imperial.ac.uk  Zhenxin Zhu AIR, Tsinghua University Beihang\n",
            "University  Zhou Jiang AIR, Tsinghua University Beijing Institute of Technology  Baijun Ye AIR,\n",
            "Tsinghua University Beijing Institute of Technology  Xiaoxue Chen AIR, Tsinghua University  Yifei\n",
            "Zhang AIR, Tsinghua University UCAS  Yuantao Chen AIR, Tsinghua University CUHK (SZ)  Jian Zhao†\n",
            "EVOL Lab, Institute of AI (TeleAI), China Telecom  Hao Zhao† AIR, Tsinghua University\n",
            "zhaohao@air.tsinghua.edu.cn  PSNR: 24.485FPS: 135Storage: 523MB  PSNR: 25.976FPS: 93Storage: 734MB\n",
            "(c) Ours 10% Pruning Rate  (b) 3D Gaussian Splatting 30k(d) Ground Truth  PSNR: 25.642FPS:\n",
            "128Storage: 104MB(a) 3D Gaussian Splatting 7k  H𝐿 = 𝐼 +  𝐴 𝜆0  = 𝑉 (𝐼 + Λ/𝜆0)𝑉 −1.  (3)  (4)  (5)\n",
            "Then we have the response of the input signal 𝑥 corresponding  3.2.2 Graph Construction. Given a set\n",
            "of Gaussian Primitives 𝑃, we want to construct a nearest neighbor graph. The adjacent matrix  to\n",
            "filters and the response magnitude could be written as:  𝜋𝑖 = ||𝑓𝑖 ||2 2  .  (6)  SUNDAE: Spectrally\n",
            "Pruned Gaussian Fields with Neural Compensation  Controlling bandwidth with 𝛾. In implementation, we\n",
            "prune the abundant primitives according to the response magnitude of the high-pass filter. We sample\n",
            "total 𝑘% of all primitives, among where 𝛾 high-frequency primitives and (1 − 𝛾) low-frequency\n",
            "primitives by querying the top 𝛾 highest magnitude and the top (1−𝛾)% lowest magnitude respectively.\n",
            "The value of 𝛾 is ablated in Section 4.4 and we used 𝛾 = 0.5 in main experiments to maintain\n",
            "consistency.<|eot_id|><|start_header_id|>user<|end_header_id|>  What is the name of the university\n",
            "Runyi Yang worked at when they published the paper titled: SUNDAE: Spectrally Pruned Gaussian Fields\n",
            "with Neural Compensation? Do not provide any information other than the\n",
            "name.<|eot_id|><|start_header_id|>assistant<|end_header_id|> Tsinghua University\n",
            "CPU times: user 1.51 s, sys: 4.86 ms, total: 1.52 s\n",
            "Wall time: 1.52 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_pdf.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 4, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"What is the name of the university Runyi Yang worked at when they published the paper titled: SUNDAE: Spectrally Pruned Gaussian Fields with Neural Compensation? Do not provide any information other than the name.\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isaD-BvlfdoD",
        "outputId": "80025371-2044-4c5c-bdfa-7049a9c6dfbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to Answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  4 2 0 2  y a M 1  ] h p - t n a u q [  1 v 3 7 6 0 0 . 5 0 4 2 : v i X r a  Quantum\n",
            "algorithms for matrix geometric means  Nana Liu,1, 2, 3, ∗ Qisheng Wang,4, † Mark M. Wilde,5, ‡ and\n",
            "Zhicheng Zhang6, § 1Institute of Natural Sciences, School of Mathematical Sciences, MOE-LSC,\n",
            "Shanghai Jiao Tong University, Shanghai, 200240, P. R. China 2Shanghai Artificial Intelligence\n",
            "Laboratory, Shanghai, China 3University of Michigan-Shanghai Jiao Tong University Joint Institute,\n",
            "Shanghai 200240, China 4Graduate School of Mathematics, Nagoya University, Nagoya 464-8602, Japan\n",
            "5School of Electrical and Computer Engineering, Cornell University, Ithaca, New York 14850, USA\n",
            "6Centre for Quantum Software and Information, University of Technology Sydney, Ultimo, NSW 2007,\n",
            "Australia (Dated: May 2, 2024)  While most of the applications introduced above are for quantum\n",
            "problems for which there is no direct classical equivalent (although the quantum learning algorithm\n",
            "can also be applied to learning Euclidean distances for classical data), there are potential\n",
            "benefits that the new quantum subroutine can have over purely classical methods. This could be\n",
            "exploited for future applications. For example, classical numerical algorithms to compute the matrix\n",
            "geometric mean have cost O(poly D) for D × D matrices [51, 52]. The same is also true for solving\n",
            "the differential matrix Riccati equation and algebraic matrix Riccati equation [18] through\n",
            "iterative methods and other methods based on finding the eigendecomposition of a larger matrix [67].\n",
            "For quantum processing on the other hand, we showed conditions under which the block-encodings of\n",
            "some of these solutions can be obtained with cost O(poly log D). For example, there are many\n",
            "classical problems for which it is important to compute the matrix geometric mean  Matrix geometric\n",
            "means between two positive definite matrices can be defined equivalently from distinct perspectives\n",
            "– as solutions to certain nonlinear systems of equations, as points along geodesics in Riemannian\n",
            "geometry, and as solutions to cer- tain optimisation problems. This diversity already suggests the\n",
            "potential for varied applications, as well as acting as a bridge between different domains. Here we\n",
            "devise new quantum subroutines to efficiently prepare quantum unitary operators that em- bed the\n",
            "standard matrix geometric mean and its generalisations called the weighted matrix geometric mean.\n",
            "This enables the construction of solutions to the algebraic Riccati equation, which is an important\n",
            "class of nonlinear systems of equations that appears in machine learning, optimal control,\n",
            "estimation, and filtering. Using these subroutines, we present a new class of quantum learning\n",
            "algorithms called quantum geometric mean metric learning. This has applications in efficiently\n",
            "finding the best distance measure  A, U †  B, U †  9  (17)  (18)  be considered as density matrices.\n",
            "Then from Lemma 30 we can prepare block-encodings UA, UB, UC by access to the unitaries that prepare\n",
            "purifications of A, B, and C, with only a single query to each purification and O(log D) gates. In\n",
            "more general scenarios, we can leave the preparation of these block-encodings to a later stage,\n",
            "which also depends on the particular application. Below, κA and κC denote the condition numbers for\n",
            "A and C, respectively. It is important to clarify that, as assumed in [1], all of our quantum\n",
            "algorithms for matrix geometric means assume that  I ≥ A ≥ I/κA, I ≥ C ≥ I/κC, I ≥ B†B.  This means\n",
            "that κA and κC are really equal to the inverses of the minimum eigenvalues of A and C, respectively,\n",
            "and ∥A∥ , ∥B∥ , ∥C∥ ≤ 1. The upper bounds above are automatically satisfied whenever A and C are\n",
            "density matrices.  A. Quantum subroutine for matrix geometric means  As a warm-up, we present a\n",
            "quantum subroutine for implementing block-encodings\n",
            "of<|eot_id|><|start_header_id|>user<|end_header_id|>  Who authored the paper titled: Quantum\n",
            "algorithms for matrix geometric means?<|eot_id|><|start_header_id|>assistant<|end_header_id|> The\n",
            "authors of the paper \"Quantum algorithms for matrix geometric means\" are:  1. Nana Liu 2. Qisheng\n",
            "Wang 3. Mark M. Wilde 4. Zhicheng Zhang\n",
            "CPU times: user 10.6 s, sys: 33.6 ms, total: 10.7 s\n",
            "Wall time: 10.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "query = \"Who authored the paper titled: Quantum algorithms for matrix geometric means?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgUFV7nti60t",
        "outputId": "90f8aa37-fbb4-4e65-c21b-88f30e3fea9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to Answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  LrG+ r Tr,Br  in analogy with Deﬁnition 3.2.  20  Theorem 7.2. Let (θ, Tr, Br) be split-\n",
            "generic. For any triple (θ′, T′  hR  LrG+ r Tr ,Br  (θ), R  LrG+ r T′ r ,B′ r  (θ′)i(Lr G0+:r+)σ =\n",
            "r, B′ hθ, ad(w)∗θ′iTσ r .  r),  Xw∈WLr G+  r  (Tr ,T′  r)σ  Proof. The properties of parahoric\n",
            "Deligne–Lusztig induction presented in Section 3 have and the proofs go through with only notational\n",
            "changes. The direct analogues for R same is the case for Section 4 and especially Theorem 4.5, the\n",
            "generic Mackey formula. Here, note that the generalized Bruhat decomposition in Section 4.2 should\n",
            "be intersected with LrG+ r ⊂ Gr. The crux then is to see that the ﬁber calculations in Section 5.\n",
            "But this is again straightforward—since the Drinfeld stratiﬁcation on X Gr is deﬁned by pullback\n",
            "from a Tr,Br stratiﬁcation on X G0 , the ﬁber cohomology calculations required to establish Theorem\n",
            "5.2 for a Drinfeld stratum is a special case of Theorem 5.1. (A particular case to keep in  LrG+ r\n",
            "Tr,Br  T0,B0  TrG+ r Tr,Br  0 ⊂ X G0  T = G−1  ⊆  G0  (  G1  (  · ·  (  Gd−1  (  Gd = G  (  (  (  (\n",
            "=  B = P−1  ⊆  P0  (  P1  (  · ·  (  Pd−1  (  G  Deﬁne for 0 ≤ i ≤ d:  Gi ri r Tri  (φ−1, . . . ,\n",
            "φi; ~P) = Inf  Giσ ri Giσ ri−1 Å  R  Gi−1 ri−1 r Tri−1 ri−1 Å  Gi ri−1 Gi−1 ri−1 ,Pi−1  (φ−1, . . .\n",
            ", φi−1; ~P)  ãã  ⊗ φi.  We write  rGr Tr  (~φ; ~P) := r  Gd rd Trd  (φ−1, . . . , φd; ~P).  4.2.\n",
            "Generic Mackey formula. Set S(Tr, Mr) = {x ∈ Gr(Fq) : x−1Trx ⊂ Mr}. We have an identiﬁcation\n",
            "Tr(Fq)\\S(Tr, Mr)/Tr(Fq) ∼= T0(Fq)\\S(T0, M0)/T0(Fq) and the generalized Bruhat decomposition  G0 =\n",
            "U0 ˙wM0N0  w∈T0(Fq)\\S(T0,M0)/M0(Fq) G  pulls back to a decomposition  Gr =  Gr,w,\n",
            "w∈T0(Fq)\\S(Tr,Mr)/M0(Fq) G  where  Gr,w := Ur ˙wMrNr = BrKw,0+:r+ ˙wPr,  Kw := U− ∩ ˙wN− ˙w−1.  The\n",
            "main theorem of this section will be a formula relating the parahoric Lusztig and  Deligne–Lusztig\n",
            "inductions RGr  and RGr  .  Mr ⊂Pr  Tr⊂Br  Theorem 4.5 (Generic Mackey formula). Let ρ be any\n",
            "representation of Mσ (M, G)-generic. Then  r which is  ∗RGr  Tr,Br  RGr  Mr,Pr  (ρ) =  ∗RMr  Inf  Gσ\n",
            "r Gσ s  RGr  Tr ,Br  RGs  Ts,Bs  (θ) = Ind  (θ) = Ind  = Ind  = Ind  Gσ r Qσ r Gσ s Qσ s Gσ r r Gσ\n",
            "Qσ  Qσ r Lσ r Qσ s Lσ s  (RLr  (θ)))  (Inf  Tr ,Lr∩Br  (RLs Ts,Ls∩Bs Qσ r Gσ Lσ s Qσ r Gσ Lσ r  (Inf\n",
            "(θ)))  (RLs  s+:r+  (Inf  Ts,Ls∩Bs  s+:r+  Gσ r r Gσ Qσ  (RLr  s+:r+  (Inf  Tr ,Lr∩Br  s+:r+  (θ)))\n",
            "(θ))),  where s = r − 1 and the last equality holds by Theorem 5.2. This shows that  dim RGr  Tr ,Br\n",
            "On the other hand, (10) implies that  r:r+| · dim RGs  (θ) = |U σ  Ts,Ts  (θ).  dim H i (14)\n",
            "Therefore |A| = |U σ  c(Xr, Qℓ)Tσ c r:r+|, which now completes the proof of Theorem 5.1.  r:r+ = |A|\n",
            "· dim H i−2N  (Xr−1, Qℓ),  for all i ≥ 0.  6. The scalar product formula for parahoric\n",
            "Deligne–Lusztig induction  × ℓ be any character. Then the restriction θ|Tσ  Deﬁnition 6.1. Let θ :\n",
            "Tσ agrees with the restriction of a (M, G)-generic character for some Levi subgroup M of G. We say\n",
            "that (θ, Tr) is split-generic if T is elliptic over F as a torus of M.  r → Q  r:r+  We prove the\n",
            "main theorem of the paper:  r, B′ Corollary. For T elliptic, RGr  r) for which the Scalar Product\n",
            "Formula does not hold. We note:  (θ) is irreducible if and only if StabWGr (Fq )(Tr )(θ) = {1}.\n",
            "Tr,Br  The new approach to the Scalar Product Conjecture presented in this paper is to describe RGr\n",
            "(θ) in terms of a sequence of parahoric Lusztig inductions associated to a(ny) Howe factorization of\n",
            "θ. The present paper appears to be the ﬁrst work to deﬁne parahoric Lusztig induction, but the\n",
            "deﬁnition is a natural generalization of classical Lusztig induction [Lus76] in the parahoric\n",
            "setting of [CI21b]. We will need several general properties of parahoric Lusztig induction, which we\n",
            "establish in Section 3. Of these results, Proposition 3.7 is the most nontrivial (for example, see\n",
            "Remark 3.8 for a discussion on a proof of this fact in r = 0 that fails for r > 0).  Tr,Br  This\n",
            "description of RGr<|eot_id|><|start_header_id|>user<|end_header_id|>  Could you provide me with the\n",
            "table of contents for Charlotte Chan's paper 'The Scalar Product Formula for Parahoric\n",
            "Deligne–Lusztig Induction'?<|eot_id|><|start_header_id|>assistant<|end_header_id|> Based on the\n",
            "provided text, here is the table of content for Charlotte Chan's paper 'The Scalar Product Formula\n",
            "for Parahoric Deligne–Lusztig Induction':  1. Introduction 2. Preliminaries         * Definition\n",
            "3.2: Split-generic         * Theorem 4.5: Generic Mackey formula         * Definition 6.1: Split-\n",
            "generic 3. Properties of Parahoric Deligne-Lusztig Induction         * Theorem 7.2: Main theorem 4.\n",
            "Proof of Theorem 7.2         * Notation and definitions         * Proofs of various lemmas and\n",
            "theorems 5. Conclusion         * Corollary: Irreducibility criterion for RGr(θ)         * Discussion\n",
            "of the Scalar Product Conjecture and its relation to the paper's results  Please note that this is\n",
            "not an exhaustive or complete table of contents, as it seems that the provided text is a snippet\n",
            "from the paper and may not represent the entire document.\n",
            "CPU times: user 52.3 s, sys: 106 ms, total: 52.4 s\n",
            "Wall time: 52.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "query = \"Could you provide me with the table of contents for Charlotte Chan's paper 'The Scalar Product Formula for Parahoric Deligne–Lusztig Induction'?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Irrelevant prompt --> test for hallucinations"
      ],
      "metadata": {
        "id": "6ayIkSUCQGuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"Accoridng to her paper, what is Charlotte Chan's favorite color?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "id": "-rtCXu7Y4bBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee83457c-d3dd-443c-d228-17aac3ad998c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:330: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='prompt formed by material and attribute (e.g., “smooth stones”, or “aged wood”) and two different random seeds for the diffusion prior. Finally, we add two real photographs of textures for each material type (from Adobe Stock [2023]), making a total of 80 images (66 generated and 14 real) associated to a pair of original and target prompts. Our full dataset is included in the Supplemental Material.', metadata={'source': '/content/document5.pdf'}), -0.1041230134886193), (Document(page_content='Input Texture(+1.0, +1.0)(+1.5, +1.5)(-1.0, -1.0)\"aged bricks\"\"new bricks\"\"small stones\"(\"big stones\" , \"mossy stones\")\\n\\nInput Texture+1.0+1.5-1.0\\n\\nFig. 9. Generalization and multi-dimensional edits. Left: We show that using directions computed for bricks (“aged” → “new”) generalize to other classes such as wood. Right: We evaluate the combination of multiple editing directions (“small stones” → “big stones” and “mossy stones”). We show here the diagonal of the matrix of edits, where both directions are changed by the same amount (see complete matrices of edits and additional results in the Supplemental Material).\\n\\n9\\n\\n\"aged wood\"\"new wood\"\\n\\nInput Texture+1.0+1.5-1.0\"fabric with vertical stripes\"\"fabric withhorizontal stripes\"\\n\\n\"stones\"\"mossy stones\"', metadata={'source': '/content/document5.pdf'}), -0.10805936055214294), (Document(page_content='One must note that the notion of identity in textures is not a formally defined one, even if it is arguably easy to judge qualitatively. For our purposes, identity depends on the input texture and the attributes being edited, and it includes all other attributes of the input texture except the ones being edited. We leave an in-depth study of texture identity in diffusion-based models for future work. We focus on color textures in this work, from which material maps could be approximated [Sartor and Peers 2023; Vecchio et al. 2023]. Extending our editing approach to such material maps should be possible, although current material diffusion models rely on a relatively limited amount of data (orders of magnitude smaller than for images), which would limit their expressivity. Better understanding the CLIP embedding space for images [Zhou et al. 2023b], and for the particular case of textures [Deschaintre et al. 2023; Wu and Maji 2022], is also an interesting line of research, both for synthesis and editing.', metadata={'source': '/content/document5.pdf'}), -0.11452628869395731), (Document(page_content='3\\n\\n<end_text><start_text>atextureofsmallstones\\n\\n<start_text>acutepandaeatingpizza<end_text>\\n\\nFig. 2. Visualization of cross-attention maps. We show maps at the last diffusion step of SD 1.4 [Rombach et al. 2022], given two different input prompts. Top: “a cute panda eating pizza” (non-texture). Bottom: “a texture of small stones” (texture). The attention maps contain interesting semantic information for the panda image, but fail to capture the texture structure.\\n\\nInputText conditioningImage conditioningGenerated images\\n\\n\"wood\"\"rough log wood, cracked and aged, with some streaks\"', metadata={'source': '/content/document5.pdf'}), -0.11724248449309038)]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.2\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>  Use the following context to Answer the\n",
            "question at the end. Do not use any other information. If you can't find the relevant information in\n",
            "the context, just say you don't have enough information to answer the question. Don't try to make up\n",
            "an answer.  <|eot_id|><|start_header_id|>user<|end_header_id|>  Accoridng to her paper, what is\n",
            "Charlotte Chan's favorite color?<|eot_id|><|start_header_id|>assistant<|end_header_id|> I don't have\n",
            "enough information to answer the question. The provided context does not mention Charlotte Chan or\n",
            "her favorite color.\n",
            "CPU times: user 6.11 s, sys: 24.9 ms, total: 6.13 s\n",
            "Wall time: 6.21 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzRDqF2ASXUf"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc79a48161a94787a68ba6f45d4ab0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4b0fd038465494ab7829fca7675efb7",
              "IPY_MODEL_55000a1e9c6942c2adfa0a005d542dd0",
              "IPY_MODEL_3856536bedc84df8aee1190f0989496d"
            ],
            "layout": "IPY_MODEL_230cde4b359d4ac58429e52ec70ebeb8"
          }
        },
        "b4b0fd038465494ab7829fca7675efb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_722824c817164935a178c51182d6c698",
            "placeholder": "​",
            "style": "IPY_MODEL_3afe89a6bbf34503bd4a82f705de1e18",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "55000a1e9c6942c2adfa0a005d542dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_309b06e039cd413484ba7b4739f20307",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51a4440f95ec4f3993e3ccae3e7697af",
            "value": 4
          }
        },
        "3856536bedc84df8aee1190f0989496d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bf7270b096d424986717788035e5bbd",
            "placeholder": "​",
            "style": "IPY_MODEL_a3eecc4a0beb4326b0c48c90253b52ca",
            "value": " 4/4 [01:14&lt;00:00, 16.17s/it]"
          }
        },
        "230cde4b359d4ac58429e52ec70ebeb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "722824c817164935a178c51182d6c698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3afe89a6bbf34503bd4a82f705de1e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "309b06e039cd413484ba7b4739f20307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51a4440f95ec4f3993e3ccae3e7697af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bf7270b096d424986717788035e5bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3eecc4a0beb4326b0c48c90253b52ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}