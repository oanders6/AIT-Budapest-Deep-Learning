{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oanders6/AIT-Budapest-Deep-Learning/blob/main/DeepLearningProjectMilestone2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Lyrics"
      ],
      "metadata": {
        "id": "oxmKGfJKy5NT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PART 1: Load Data**"
      ],
      "metadata": {
        "id": "VKheYK_4y6R8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get data from Kaggle Datasource"
      ],
      "metadata": {
        "id": "8cHeqCX4nOso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz2AchaVzJ9D",
        "outputId": "50909c34-a396-463a-8531-2d5d70292bc5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npH_cURPzcQi",
        "outputId": "8b1156cb-9b8b-4731-a071-ae1d5c28e6ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "f01tYh7o16bg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "h7vQqZG24DZw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hyEPFZqU4gVO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d juicobowley/drake-lyrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgDyG1TP49Os",
        "outputId": "c064c6d4-e4ed-467a-9568-088d65ddf3cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading drake-lyrics.zip to /content\n",
            "\r  0% 0.00/764k [00:00<?, ?B/s]\n",
            "\r100% 764k/764k [00:00<00:00, 121MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d suraj520/music-dataset-song-information-and-lyrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DDbBHqiou7K",
        "outputId": "0ba9f103-729d-444e-a34d-415c7a60f31c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading music-dataset-song-information-and-lyrics.zip to /content\n",
            "\r  0% 0.00/1.90M [00:00<?, ?B/s]\n",
            "\r100% 1.90M/1.90M [00:00<00:00, 126MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip drake-lyrics.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bED2ZFPb5uDi",
        "outputId": "2132c253-8ea0-45f1-e48c-869f274036ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drake-lyrics.zip\n",
            "  inflating: drake_data.csv          \n",
            "  inflating: drake_data.json         \n",
            "  inflating: drake_lyrics.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip music-dataset-song-information-and-lyrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Egi5DgIo1dW",
        "outputId": "0d5cc145-7411-4702-e264-108851d26f92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  music-dataset-song-information-and-lyrics.zip\n",
            "  inflating: songs.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse the lyrics to get a list of words"
      ],
      "metadata": {
        "id": "2Se5n2WQnV3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "# NOTE:\n",
        "# need to download the 'punkt' and 'stopwords' depndencies from nltk, used by\n",
        "# tokenize and stopworks respectively:\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRPyTqBeAXeO",
        "outputId": "bb6b5503-0f9c-40e2-ce8b-bde8f0a2114f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "other_dataset = pd.read_csv('/content/songs.csv')\n",
        "drake_dataset = pd.read_csv('/content/drake_data.csv')"
      ],
      "metadata": {
        "id": "tpS8IMQopSjJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drake_lyrics = drake_dataset.lyrics"
      ],
      "metadata": {
        "id": "xYplvULZ7C8i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other artists"
      ],
      "metadata": {
        "id": "luro7HnEp9aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_lyrics_with_title_format(title, lyrics2):\n",
        "    # Extract the first line of lyrics\n",
        "    first_line = lyrics2.split('\\n')[0]\n",
        "\n",
        "    # Apply the regex pattern to the first line of lyrics\n",
        "    if re.search(fr'\\b{re.escape(title)}\\b', lyrics2, flags=re.IGNORECASE):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def remove_first_line(text):\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Join the lines, excluding the first one\n",
        "    new_text = '\\n'.join(lines[1:])\n",
        "\n",
        "    return new_text\n",
        "def filter_dataset_with_title_format(dataset):\n",
        "    # Initialize a list to store filtered rows\n",
        "    filtered_rows = []\n",
        "\n",
        "    # Iterate over each row of the dataset\n",
        "    for index, row in dataset.iterrows():\n",
        "        # Extract title and lyrics from the current row\n",
        "        title = row['Name']\n",
        "        lyrics2 = row['Lyrics']\n",
        "\n",
        "        # Check if the lyrics match the title format\n",
        "        if filter_lyrics_with_title_format(title, lyrics2):\n",
        "            # If match found, add the row to filtered rows\n",
        "            filtered_rows.append(row)\n",
        "\n",
        "    # Create a new DataFrame with the filtered rows\n",
        "    filtered_dataset = pd.DataFrame(filtered_rows)\n",
        "\n",
        "    return filtered_dataset"
      ],
      "metadata": {
        "id": "ifm2873spvdx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drake Lyrics"
      ],
      "metadata": {
        "id": "4zXZcUfhqDGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  if isinstance(text, str):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove anything in brackets or parenthesis\n",
        "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "  else:\n",
        "    return ''"
      ],
      "metadata": {
        "id": "INpa1XPH7E4s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_only = filter_dataset_with_title_format(other_dataset)\n",
        "\n",
        "lyrics_only['Lyrics'] = lyrics_only['Lyrics'].apply(remove_first_line)\n",
        "\n",
        "lyrics_only.head()\n",
        "\n",
        "lyrics_only['Lyrics'] = lyrics_only['Lyrics'].apply(preprocess_text)\n",
        "\n",
        "lyrics2 = []\n",
        "for i in lyrics_only['Lyrics']:\n",
        "  lyrics2 += [i]\n",
        "\n",
        "print(lyrics2[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1CG3NUxp7WP",
        "outputId": "e1c8a45e-76f9-4e97-8f2c-36a745df7511"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sweet', 'lord', 'mmm', 'lord', 'mmm', 'lord', 'really', 'wan', 'na', 'see', 'really', 'wan', 'na', 'really', 'wan', 'na', 'see', 'lord', 'takes', 'long', 'lord', 'sweet', 'lord', 'mmm', 'lord', 'mmm', 'lord', 'really', 'wan', 'na', 'know', 'id', 'really', 'wan', 'na', 'go', 'really', 'wan', 'na', 'show', 'lord', 'wont', 'take', 'long', 'lord', 'sweet', 'lord', 'mmm', 'lord', 'sweet', 'lord', 'might', 'also', 'like', 'really', 'wan', 'na', 'see', 'really', 'wan', 'na', 'see', 'really', 'wan', 'na', 'see', 'lord', 'really', 'wan', 'na', 'see', 'lord', 'takes', 'long', 'lord', 'sweet', 'lord', 'mmm', 'lord', 'lord', 'really', 'wan', 'na', 'know', 'really', 'wan', 'na', 'go', 'really', 'wan', 'na', 'show', 'lord', 'wont', 'take', 'long', 'lord', 'mmmmm', 'sweet', 'lord', 'lord', 'mmm', 'lord', 'lord', 'oh', 'oh', 'sweet', 'lord', 'ooh', 'really', 'wan', 'na', 'see', 'really', 'wan', 'na', 'really', 'wan', 'na', 'see', 'lord', 'takes', 'long', 'lord', 'mmm', 'lord', 'lord', 'sweet', 'lord', 'sweet', 'lord', 'ah', 'oh', 'lord', 'mmmmm', 'mmmmm', 'mmmmm', 'mmmmm', 'sweet', 'lord', 'sweet', 'lord', 'lord', 'lord', 'sweet', 'lord', 'sweet', 'lord', 'sweet', 'lord', 'sweet', 'lord', '23embed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "processed_lyrics = preprocess_text(drake_lyrics[0])\n",
        "processed_lyrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIoBLtyi8Lcb",
        "outputId": "8f76e8ad-b389-451b-f819-e183a1610ff3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['put',\n",
              " 'feelings',\n",
              " 'ice',\n",
              " 'always',\n",
              " 'gem',\n",
              " 'certified',\n",
              " 'lover',\n",
              " 'boy',\n",
              " 'somehow',\n",
              " 'still',\n",
              " 'heartless',\n",
              " 'heart',\n",
              " 'gettin',\n",
              " 'colder']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first song's processed lyrics without stop words"
      ],
      "metadata": {
        "id": "fXpTm6s0nctJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create a list of all of the lyrics and tokenize them"
      ],
      "metadata": {
        "id": "wkZ1PAPPnkve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_of_song_marker = ['ENDOFSONG']\n",
        "tokens = []\n",
        "for i in range(0, len(drake_lyrics)):\n",
        "  if i != 213:\n",
        "    tokens += preprocess_text(drake_lyrics[i])\n",
        "    tokens += end_of_song_marker\n",
        "\n",
        "tokens2 = []\n",
        "for i in range(0, len(lyrics2)):\n",
        "  tokens2 += lyrics2[i]\n",
        "  tokens2 += end_of_song_marker"
      ],
      "metadata": {
        "id": "WwQPJJuSAuGb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Join the tokens back into a single string\n",
        "text = ' '.join(tokens)\n",
        "text2 = ' '.join(tokens2)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer2 = Tokenizer()\n",
        "\n",
        "# Fit tokenizer on the text\n",
        "tokenizer.fit_on_texts([text])\n",
        "tokenizer2.fit_on_texts([text2])\n",
        "\n",
        "# Tokenize the list of tokens\n",
        "tokenized_sequence = tokenizer.texts_to_sequences([tokens])[0]\n",
        "tokenized_sequence2 = tokenizer2.texts_to_sequences([tokens2])[0]"
      ],
      "metadata": {
        "id": "HIaD45OUD4Gy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens), len(tokenized_sequence), len(tokens2), len(tokenized_sequence2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9ekh3Xefhsz",
        "outputId": "ef0329bd-4d54-4ede-ee74-6a9e86529148"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76128 76128 102142 102142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create sequences from each lyric to the next, stopped by the end of the song"
      ],
      "metadata": {
        "id": "81AJlk_qn0w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_data(tokens, tokenized_sequence):\n",
        "  sequences = []\n",
        "\n",
        "  for i in range (0, len(tokenized_sequence) - 3):\n",
        "      curr  = tokens[i]\n",
        "      next1 = tokens[i+1]\n",
        "      next2 = tokens[i+2]\n",
        "      next3 = tokens[i+3]\n",
        "      # If neither token is the end-of-song marker, add it to the current sequence\n",
        "      if curr != 'ENDOFSONG' and next1 != 'ENDOFSONG' and next2 != 'ENDOFSONG' and next3 != 'ENDOFSONG':\n",
        "          sequences.append([tokenized_sequence[i], tokenized_sequence[i+1], tokenized_sequence[i+2], tokenized_sequence[i+3]])\n",
        "\n",
        "\n",
        "  # Convert sequences to numpy array\n",
        "  sequences = np.array(sequences)\n",
        "\n",
        "  # Split sequences into input (X) and output (Y)\n",
        "  X = sequences[:, :-1]\n",
        "  Y = sequences[:, -1]\n",
        "\n",
        "  # Split the dataset into training (80%), validation (10%), and test (10%) sets\n",
        "  X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2)\n",
        "  X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5)\n",
        "\n",
        "  return X_train, X_val, X_test, Y_train, Y_val, Y_test"
      ],
      "metadata": {
        "id": "TQfQSpksLY7Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_drake, X_val_drake, X_test_drake, Y_train_drake, Y_val_drake, Y_test_drake = split_data(tokens, tokenized_sequence)"
      ],
      "metadata": {
        "id": "a63XLZ-Si2yd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_other, X_val_other, X_test_other, Y_train_other, Y_val_other, Y_test_other = split_data(tokens2, tokenized_sequence2)"
      ],
      "metadata": {
        "id": "kbsHbR_CjBqp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train_other), len(X_val_other), len(X_test_other))\n",
        "print(len(Y_train_other), len(Y_val_other), len(Y_test_other))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSmMXdqpxh9f",
        "outputId": "fb41396e-09c2-4df1-d418-e8d37e297388"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80174 10022 10022\n",
            "80174 10022 10022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train_drake), len(X_val_drake), len(X_test_drake))\n",
        "print(len(Y_train_drake), len(Y_val_drake), len(Y_test_drake))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab18hAelxphN",
        "outputId": "622f9cad-f572-4c01-9e27-e15298866dee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59982 7498 7498\n",
            "59982 7498 7498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# Define the LSTM model\n",
        "def create_model(vocabulary_size, seq_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocabulary_size, 50, input_length=seq_length))\n",
        "    model.add(LSTM(100, return_sequences=True))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Train the LSTM model\n",
        "def train_model(X_train, Y_train, X_val, Y_val, vocab_size):\n",
        "    model = create_model(vocab_size, X_train.shape[1])\n",
        "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=300)\n",
        "    return model\n",
        "\n",
        "# Evaluate the trained model on the test set\n",
        "def evaluate_model(model, X_test, Y_test):\n",
        "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test Accuracy: %.2f%%' % (accuracy*100))\n",
        "\n",
        "# For Drake's lyrics\n",
        "model_drake = train_model(X_train_drake, Y_train_drake, X_val_drake, Y_val_drake, len(tokenizer.word_index)+1)\n",
        "evaluate_model(model_drake, X_test_drake, Y_test_drake)\n",
        "\n",
        "# For other dataset lyrics\n",
        "model_other = train_model(X_train_other, Y_train_other, X_val_other, Y_val_other, len(tokenizer2.word_index)+1)\n",
        "evaluate_model(model_other, X_test_other, Y_test_other)"
      ],
      "metadata": {
        "id": "WOIvW3YCx9Qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180d5fbf-4b91-4273-8a8d-0cee152a902a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 50)             424150    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 100)            60400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8483)              856783    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1431833 (5.46 MB)\n",
            "Trainable params: 1431833 (5.46 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 34s 146ms/step - loss: 7.6119 - accuracy: 0.0238 - val_loss: 7.3105 - val_accuracy: 0.0289\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 31s 153ms/step - loss: 7.1471 - accuracy: 0.0268 - val_loss: 7.3564 - val_accuracy: 0.0289\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 29s 147ms/step - loss: 7.1104 - accuracy: 0.0268 - val_loss: 7.3967 - val_accuracy: 0.0291\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 29s 148ms/step - loss: 7.0838 - accuracy: 0.0268 - val_loss: 7.4320 - val_accuracy: 0.0285\n",
            "Epoch 5/10\n",
            "  8/200 [>.............................] - ETA: 23s - loss: 7.0438 - accuracy: 0.0229"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8Wldocc3Pc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}